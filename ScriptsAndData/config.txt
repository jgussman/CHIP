#
##----------
### CHIP.py 
##----------
#

# Do you want to run CHIP?
run_CHIP_q = True

# Cross Match Names file path ---> this txt file must have two columns seperated by a space with headers, one 
#                                  of the columns will the names of the stars in the catalogue you want. The 
#                                  other column must have the column name as "HIRES" and contain the names of
#                                  the HIRES identification for the stars. 
crossMatchStarsFilePath = ../SPOCSdata/starnames_crossmatch_SPOCS_NEXSCI.txt

# Alpha Normalization ---> Would you like to use the alpha normalization techinque to normalize your spectra?
alpha_question = True

# Use Past Data ---> Would you like to use the data CHIP produced in your last run of CHIP.py? 
past_data_question = False 



#
##----------
### main_The_Cannon.py
##----------
#

# N = number of pixels in wavelength file 
# S = number of stars 
# A = number of abudances (labels)
# X = doesn't matter the number 

# Do you want to train The Cannon Models? 
cannon_q = False

# Wavelengths for all stars: shape = (N,)
wavelength_file_path = interpolated_wl.csv

# Flux for each individual star: shape = (S,N)
fluxes_file_path = fluxes_for_HIRES.npy

# Inverse variance (1/sigma**2) for each individual star: shape = (S,N)
ivar_file_path = ivars_for_HIRES.npy

# The star's identification/name: shape = (S,)
id_file_path = stellar_names_for_flux_and_ivar.npy

# Masks to apply to all stars; for example: [("Name of Mask","file Path to mask"),...,("No Mask",False)], shape of mask = (N,)
# ,
masks_list = [("No Mask",False),("iodine","../Constants/Masks/by_eye_iodine_mask.npy")]  

#Abudances for all the parameters: dataframe with shape = (X,A)
# The 0th column will be converted to the index!!! This means the 0th column needs to be the identifier corresponding 
# to the 0th column of the file3 you put in for crossMatchedNames in main_pipeline.py
abundances_file_path = ../SPOCSdata/df_all.csv

# Parameter names that reflect the spelling in the file for abundances_file_path
# 
parameters = ['TEFF', 'LOGG','VSINI', 'FeH', 'CH', 'NH','OH','NaH', 'MgH', 'AlH', 'SiH', 'CaH', 'TiH', 'VH', 'CrH', 'MnH','NiH', 'YH']  

# Random seed for replication
random_seed = 3

# Number of parameters to train at the same time: must be a list of int values and cannot be left empty
# 
group_sizes_list = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18] 

# The % of the stars that will be used as the test set (int)
testing_percentage = 80

# The % of the training stars that will be used as the validation set (int)
validation_percentage = 15

# Name and function for computing loss for model
loss_metric_name = MSE
loss_metric_fun = lambda true_array,predicted_array:  np.mean(true_array - predicted_array)
