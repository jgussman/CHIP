{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from hiresprv.auth import login\n",
    "from hiresprv.idldriver import Idldriver\n",
    "from hiresprv.database import Database\n",
    "from hiresprv.download import Download\n",
    "from PyAstronomy import pyasl\n",
    "from specutils.fitting import fit_generic_continuum\n",
    "from specutils.spectra import Spectrum1D\n",
    "from scipy import interpolate \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "class CIR:\n",
    "\n",
    "    def __init__(self,star_ID_array,wl_solution_path = './wl_solution.csv',rvOutputPath='./RVOutput',spectraOutputPath = './SpectraOutput' ):\n",
    "        '''\n",
    "        Used for reducing HIRES data to get it ready for being fed into The Cannon. \n",
    "\n",
    "        star_ID_array: is an array that contains strings of the HIRES ID for stars\n",
    "        wl_solution_path: defaulted to ./wl_solution.csv, the file at the end of your given path\n",
    "                          needs to have the first 2 rows not contain data. skip_header is set to 2\n",
    "        rvOutputPath: defaulted to ./RVOutput, the folder at the end of your path needs to have already\n",
    "                      been created before you initalize this class\n",
    "        spectraOutputPath: defaulted to ./SpectraOutput, the folder at the end of your path needs to have \n",
    "                            already been created before you initalize this class\n",
    "        '''\n",
    "\n",
    "        login('prv.cookies')                                               # For logging into the NExSci servers \n",
    "        self.idldriver = Idldriver('prv.cookies')                          # For creating RV scripts \n",
    "        self.state = Database('prv.cookies')                               # For retrieving data from HIRES \n",
    "        self.dataRV = Download('prv.cookies', rvOutputPath)                # For downloading RV\n",
    "        self.dataSpectra = Download('prv.cookies',spectraOutputPath)       # For downloading Spectra \n",
    "        self.star_ID_array = star_ID_array                                 # HIRES ID \n",
    "        self.wl_solution = np.genfromtxt(wl_solution_path,skip_header=2)   # UNITS: Angstrom\n",
    "        self.Ivar = {}                                                     # For storing sigma valeus \n",
    "        self.filename_rv_df = pd.DataFrame()                               # For storing meta Data\n",
    "        \n",
    "    \n",
    "    def Run(self):\n",
    "        '''\n",
    "        Description: This method will run all of the following \n",
    "             Find_and_download_all_rv_obs -> \n",
    "             DownloadSpectra ->\n",
    "             ContinuumNormalize ->\n",
    "             CrossCorrelate -> \n",
    "             Interpolate \n",
    "        '''\n",
    "        self.Find_and_download_all_rv_obs()\n",
    "        self.DownloadSpectra()\n",
    "        self.ContinuumNormalize()\n",
    "        self.CrossCorrelate()\n",
    "        self.Interpolate()\n",
    "\n",
    "\n",
    "    def Find_and_download_all_rv_obs(self):\n",
    "        '''            \n",
    "        Description: This method downloads the rotational velocity metadata and\n",
    "        returns a dictionary that makes it easy to identify what stars' rotational \n",
    "        velocities nearest 0 as well as the filenames for which they came from.\n",
    "\n",
    "        Note: The dataframe produced by this method will remove all the stars that did \n",
    "              not have any RV data produced by rvcurve. This does not mean that \n",
    "              the star doesn't have any RV data. It could mean a few things, the ID\n",
    "              is wrong, contains a character that isn't currently supported by the \n",
    "              HIRES pipeline, it could have too few RV observations to make an \n",
    "              RV curve, etc...\n",
    "        '''\n",
    "        print(\"Downloading RV Data Has Began\")\n",
    "        #Downloading the RV data as well as getting the largest RV value for each star\n",
    "        hiresName_fileName_rv_dic = {\"HIRESName\": [],\"FILENAME\":[], \"RV\":[]}  \n",
    "        rvDownloadLocation = self.dataRV.localdir\n",
    "        for name in self.star_ID_array:\n",
    "            #Make sure the data is in workspace\n",
    "            hiresName_fileName_rv_dic[\"HIRESName\"].append(name)\n",
    "            try:\n",
    "                rtn = self.dataRV.rvcurve(name)\n",
    "                nameLoc = '{0}/vst{1}.csv'.format(rvDownloadLocation,name)\n",
    "                temp_df = pd.read_csv(nameLoc)\n",
    "                if not temp_df.empty:\n",
    "                    rv_temp = abs(temp_df['RV'])\n",
    "                    row = temp_df[temp_df['RV'] == rv_temp.max()]\n",
    "                    if row.empty: #The absolute max rv is negative \n",
    "                        row = temp_df[temp_df['RV'] == -rv_temp.max()]\n",
    "                    hiresName_fileName_rv_dic[\"RV\"] += [row[\"RV\"].to_numpy()[0]]\n",
    "                    hiresName_fileName_rv_dic[\"FILENAME\"] += [row[\"FILENAME\"].to_numpy()[0]]\n",
    "                else:\n",
    "                    #This is for removing these RV-less stars \n",
    "                    hiresName_fileName_rv_dic[\"RV\"] += [pd.NA]\n",
    "                    hiresName_fileName_rv_dic[\"FILENAME\"] += [pd.NA]\n",
    "            except OSError: #This error occurs because for some reason the star's rvcurve wasn't created\n",
    "                    #This is for removing these stars that have no RV metadata  \n",
    "                    hiresName_fileName_rv_dic[\"RV\"] += [pd.NA]\n",
    "                    hiresName_fileName_rv_dic[\"FILENAME\"] += [pd.NA]\n",
    "        df = pd.DataFrame(hiresName_fileName_rv_dic) \n",
    "        self.filename_rv_df = df.dropna() #If you don't drop the na's then other methods will break\n",
    "        self.filename_rv_df.to_csv(\"HIRES_Filename_rv.csv\",index_label=False,index=False)\n",
    "        print(\"Downloading RV Data Has Ended\")\n",
    "        \n",
    "\n",
    "    def DownloadSpectra(self):\n",
    "        '''\n",
    "        Description: This method downloads all the deblazed spectra from HIRES that are in self.filename_rv_df[\"FILENAME\"]\n",
    "                     to self.dataSpectra.localdir\n",
    "        '''\n",
    "        print(\"Downloading Spectra Has Began\")\n",
    "        self.spectraDic = {} \n",
    "        download_Location = self.dataSpectra.localdir #This is the second parameter of hiresprv.download.Download\n",
    "        for filename in self.filename_rv_df[\"FILENAME\"]:\n",
    "            #I tried to use the , seperation and new line seperation \n",
    "            #for the different file names but it doesn't seem to work.\n",
    "            #Thus, a for-loop was used!\n",
    "            self.dataSpectra.spectrum(filename.replace(\"r\",\"\"))  #Download spectra \n",
    "            file_path = \"{0}/{1}.fits\".format(download_Location,filename)\n",
    "            temp_deblazedFlux = fits.getdata(file_path)\n",
    "            #spectraDic is the deblazed spectra  \n",
    "            self.spectraDic[filename] = np.append(temp_deblazedFlux[0],\n",
    "                                                  [temp_deblazedFlux[i] for i in range(1,16)])     \n",
    "            self.SigmaCalculation(filename) #To get the sigma for Inverse variance  \n",
    "        print(\"Downloading Spectra Has Ended\")\n",
    "\n",
    "    def ContinuumNormalize(self):\n",
    "        '''        \n",
    "        Description: This method uses specutils' Spectrum1D function to fit a function\n",
    "                     to each echelle order spectra then subtracts the function from the \n",
    "                     echelle order. \n",
    "                     The normalized spectra are put in the {} called self.spectraDic with\n",
    "                     the keys being the HIRES filename and the values being a continuum \n",
    "                     normalized 1-D array.   \n",
    "        '''\n",
    "        #This is the same for all HIRES data \n",
    "        print(\"Continuum Normalization Began\")\n",
    "    \n",
    "        spectral_axis_wl_solution = self.wl_solution*u.um\n",
    "        length = len(spectral_axis_wl_solution)\n",
    "        download_Location = self.dataSpectra.localdir \n",
    "        #Continumm Normalize\n",
    "        for filename in self.spectraDic:\n",
    "            deblazedFlux = self.spectraDic[filename]*u.Jy\n",
    "            ivarSigma = self.Ivar[filename]*u.Jy\n",
    "            normalized_array = np.array([])\n",
    "            ivar_array = np.array([])\n",
    "            for j in range(0,16): #Normalize each individual echelle order \n",
    "                i  = 4021 * j #Each HIRES echelle order has 4021 elements \n",
    "                temp_echelle_wl,temp_echelle_flux = spectral_axis_wl_solution[i:i+4021],deblazedFlux[i:i+4021]\n",
    "                temp_sigma  =  ivarSigma[i:i+4021]\n",
    "                \n",
    "                #Use specutils' Spectrum1D to fit continuum normalize the echelle order\n",
    "                #referencing https://specutils.readthedocs.io/en/stable/spectrum1d.html\n",
    "                spectrum = Spectrum1D(flux=temp_echelle_flux, spectral_axis=temp_echelle_wl)\n",
    "                g1_fit = fit_generic_continuum(spectrum)\n",
    "                flux_fit = g1_fit(temp_echelle_wl)\n",
    "                \n",
    "                #Subracting the continuum from the echelle order & \n",
    "                #the section of sigma corresponding to the current echelle order\n",
    "                normalized_echelle = temp_echelle_flux / flux_fit\n",
    "                normalized_ivar = temp_sigma / flux_fit\n",
    "                #Converting to a float like this removes 2 decimal places from normalized_echelle\n",
    "                normalized_echelle = np.array(list(map(np.float,normalized_echelle))) \n",
    "                normalized_ivar = np.array(list(map(np.float,normalized_ivar))) \n",
    "                #Make all the echelle spectra into a 1-D array again\n",
    "                normalized_array = np.append(normalized_array,normalized_echelle)\n",
    "                ivar_array = np.append(ivar_array,normalized_ivar)\n",
    "            spec_norm_scaled = normalized_array/np.repeat(np.percentile(normalized_array, 95, axis=0)[np.newaxis], length, axis=0)\n",
    "            self.spectraDic[filename] = spec_norm_scaled #1D normalized spectra (all 16 echelle orders)\n",
    "            self.Ivar[filename] = ivar_array #Technically this isn't the inverse variacne yet\n",
    "        print(\"Continuum Normalization Ended\")\n",
    "            \n",
    "    def CrossCorrelate(self,numOfEdgesToSkip = 100):\n",
    "        '''\n",
    "        Description: Uses Pyastronomy's crosscorrRV function to compute the cross correlation.\n",
    "\n",
    "                     The updated spectra are put into self.spectraDic.   \n",
    "\n",
    "        numOfEdgesToSkip: the amount of pixels to cut off both ends of the spectra.\n",
    "        '''\n",
    "        print(\"Cross Correlate Has Began\")\n",
    "        wvlen,c = np.genfromtxt(\"../Atlases/solarAtlas.txt\",skip_header=1,usecols=(1,4),unpack=True) \n",
    "        wvlen, c = wvlen[::-1], c[::-1]\n",
    "        #We Don't need the whole solar spectra for cross correlatation the next 2-lines \n",
    "        #slice the array to only give us the wavelength range the overlaps the echelle order \n",
    "        #we are using. \n",
    "        #I compared all echelle orders' results and they only varried from one another\n",
    "        #by ~ 0.5 in the worst cases\n",
    "        start,stop = 290_000, 340_500 \n",
    "        wvlen, c = wvlen[start:stop],c[start:stop]\n",
    "        begin,end = 4021*2, 4021*3 #I'm using the 2nd echelle order\n",
    "        wl_solution = np.genfromtxt('./wl_solution.csv',skip_header=2) #UNITS: Angstrom\n",
    "        \n",
    "        crossCorrelatedspectra = {} #Key: FILENAME Values: (correlated wavelength, normalized flux)\n",
    "        for i in range(self.filename_rv_df.shape[0]):\n",
    "            row = self.filename_rv_df.iloc[i]\n",
    "            filename = row[1]\n",
    "            #RV = abs(row[2])\n",
    "            #Uncomment above if you want to use the observed RV\n",
    "            RV = 80 #60 gave good results \n",
    "            \n",
    "            normalizedFlux = self.spectraDic[filename]\n",
    "            \n",
    "            rv, cc = pyasl.crosscorrRV(wl_solution[begin:end], normalizedFlux[begin:end],wvlen,c, -1*RV, RV, RV/600., skipedge=numOfEdgesToSkip)\n",
    "            #maxind = np.argmax(cc) #DELETE IF CODE RUNS\n",
    "            argRV = rv[np.argmax(cc)]  #UNITS: km/s \n",
    "            \n",
    "            z = (argRV/299_792.458) #UNITS: None \n",
    "            computeShiftedWavelength = lambda wl: wl/ (1 + z)  #UNITS: Angstroms\n",
    "            #There has to be a better way to convert to a numpy array\n",
    "            shifted_wl = np.array(list(map(computeShiftedWavelength,wl_solution)))\n",
    "            self.spectraDic[filename] = (shifted_wl,normalizedFlux)     \n",
    "        print(\"Cross Correlate Has Ended\")\n",
    "\n",
    "    def Interpolate(self):\n",
    "        '''        \n",
    "        Description: This method downloads the interpolated wavelength to interpolated_wl.csv \n",
    "                     and downloads the fluxes to fluxes_for_HIRES.csv. \n",
    "        '''\n",
    "        print(\"Interpolation Has Began\")\n",
    "        #Interpolate the spectra with each other to get the same wavelength scale for all of them.\n",
    "        maxMinVal = float('-inf')  \n",
    "        minMaxVal = float('inf')\n",
    "        #Finds the max minimum wavelength val & finds the min maximum wavelenght val \n",
    "        for spectra_flux_tuple in self.spectraDic.values(): \n",
    "            #Assumption: wavelength is sorted from the 0th index being min,\n",
    "            #            the len(wavelength array)-1 is the max wavelength val,\n",
    "            #            all the wavelength arrays are the same length.\n",
    "            temp_spectra = spectra_flux_tuple[0]\n",
    "            temp_min_wl = temp_spectra[0]\n",
    "            temp_max_wl = temp_spectra[-1]\n",
    "            \n",
    "            if maxMinVal < temp_min_wl:\n",
    "                maxMinVal = temp_min_wl\n",
    "            if minMaxVal > temp_max_wl:\n",
    "                minMaxVal = temp_max_wl\n",
    "        \n",
    "        #Wavelength range \n",
    "        firstKey = next(iter(self.spectraDic))\n",
    "        first_spectra = self.spectraDic[firstKey][0]\n",
    "        interpolate_over = [wl for wl in first_spectra if wl >= maxMinVal and wl<= minMaxVal]   \n",
    "        length_interpolate = len(interpolate_over)\n",
    "        \n",
    "        spoc_wl = np.genfromtxt(\"../spocData/wavelengths_flat.txt\")\n",
    "        \n",
    "        spoc_wl = spoc_wl[spoc_wl >= interpolate_over[0]]\n",
    "        spoc_wl = spoc_wl[spoc_wl <= interpolate_over[-1]]\n",
    "        \n",
    "        interpolate_over = spoc_wl[::-1]\n",
    "        length_interpolate = len(interpolate_over)\n",
    "        \n",
    "        #Interpolation         \n",
    "        replacementSpectraDic = {}\n",
    "        replacementIvarDic = {}\n",
    "        \n",
    "        for HIRESname,filename,rv in self.filename_rv_df.to_numpy():\n",
    "            wl = self.spectraDic[filename][0]\n",
    "            flux_norm = self.spectraDic[filename][1]\n",
    "            flux_func = interpolate.interp1d(wl, flux_norm)\n",
    "            ivar_func = interpolate.interp1d(wl,self.Ivar[filename])\n",
    "            \n",
    "            replacementSpectraDic[HIRESname] = flux_func(interpolate_over)\n",
    "            #Now ivar will actually become ivar \n",
    "            replacementIvarDic[HIRESname] = 1/ivar_func(interpolate_over)**2 \n",
    "            \n",
    "            if np.isnan(replacementIvarDic[HIRESname]).any(): #Happens in the ivar\n",
    "                print(f\"****{HIRESname} HAS BEEN REMOVED BECAUSE IT CONTAINS NAN VALUES\")\n",
    "                del replacementSpectraDic[HIRESname]\n",
    "                del replacementIvarDic[HIRESname]\n",
    "            \n",
    "        self.spectraDic = replacementSpectraDic\n",
    "        self.Ivar = replacementIvarDic\n",
    "        self.interpolation_range = interpolate_over\n",
    "        \n",
    "        \n",
    "        #Saving Data\n",
    "        np.savetxt(\"interpolated_wl.csv\",interpolate_over,delimiter=\",\",header='wavelength(Angstrom)')\n",
    "        fluxDF = pd.DataFrame(self.spectraDic)\n",
    "        fluxDF.to_csv(\"fluxes_for_HIRES.csv\",index_label=False,index=False)\n",
    "        ivarDF = pd.DataFrame(self.Ivar)\n",
    "        ivarDF.to_csv(\"ivar_for_HIRES.csv\",index_label=False,index=False)\n",
    "\n",
    "        #This might be confusing to now make self.Ivar a dataframe when it was \n",
    "        #just a dictionary, but thats okay. Don't want to make too many variables.\n",
    "        self.Ivar = ivarDF     \n",
    "        self.fluxDF  = fluxDF\n",
    "        self.interpolate_wl = interpolate_over\n",
    "        print(\"Interpolation Has Ended\")\n",
    "\n",
    "    def SigmaCalculation(self,filename):\n",
    "        '''\n",
    "        Calculates sigma for inverse variance (ivar) \n",
    "        '''\n",
    "        gain = 1.2 #electrons/ADU\n",
    "        readn = 2.0 #electrons RMS\n",
    "        xwid = 5.0 #pixels, extraction width\n",
    "        sigma = np.sqrt((gain*self.spectraDic[filename]) + (xwid*readn**2))/gain \n",
    "        self.Ivar[filename] = sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOA userid: jgussman\n",
      "KOA Password: ········\n",
      "Successful login as jgussman\n",
      "Downloading RV Data Has Began\n",
      "Downloading RV Data Has Ended\n",
      "Downloading Spectra Has Began\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3c84f26e0950>:294: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sigma = np.sqrt((gain*self.spectraDic[filename]) + (xwid*readn**2))/gain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Spectra Has Ended\n",
      "Continuum Normalization Began\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Model is linear in parameters; consider using linear fitting methods. [astropy.modeling.fitting]\n",
      "WARNING:astropy:Model is linear in parameters; consider using linear fitting methods.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuum Normalization Ended\n",
      "Cross Correlate Has Began\n",
      "Cross Correlate Has Ended\n",
      "Interpolation Has Began\n",
      "****HD124292 HAS BEEN REMOVED BECAUSE IT CONTAINS NAN VALUES\n",
      "****HD117176 HAS BEEN REMOVED BECAUSE IT CONTAINS NAN VALUES\n",
      "Interpolation Has Ended\n",
      "This took 145.24172323942184 minutes!\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    crossMatchedNames = pd.read_csv(\"../spocData/starnames_crossmatch_SPOCS_NEXSCI.txt\",sep=\" \")\n",
    "    cirObject = CIR(crossMatchedNames[\"HIRES\"].to_numpy(),'./wl_solution.csv','./RVOutput','./SpectraOutput')\n",
    "    cirObject.Run()\n",
    "    time_elap = time.time() - start_time \n",
    "    print(f\"This took {time_elap/60} minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE EVERTHING BELOW THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to figure out when the Ivar spectra starts looking bad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOA userid: jgussman\n",
      "KOA Password: ········\n",
      "Successful login as jgussman\n"
     ]
    }
   ],
   "source": [
    "crossMatchedNames = pd.read_csv(\"../spocData/testStar.txt\",sep=\" \")\n",
    "testcirObject = CIR(crossMatchedNames[\"HIRES\"].to_numpy(),'./wl_solution.csv','./RVOutput','./SpectraOutput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading RV Data Has Began\n",
      "Downloading RV Data Has Ended\n",
      "Downloading Spectra Has Began\n",
      "Downloading Spectra Has Ended\n",
      "Continuum Normalization Began\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Model is linear in parameters; consider using linear fitting methods. [astropy.modeling.fitting]\n",
      "WARNING:astropy:Model is linear in parameters; consider using linear fitting methods.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuum Normalization Ended\n",
      "Cross Correlate Has Began\n",
      "Cross Correlate Has Ended\n",
      "Interpolation Has Began\n",
      "Interpolation Has Ended\n"
     ]
    }
   ],
   "source": [
    "testcirObject.Run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading RV Data Has Began\n",
      "Downloading RV Data Has Ended\n"
     ]
    }
   ],
   "source": [
    "# testcirObject.Run()\n",
    "testcirObject.Find_and_download_all_rv_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Spectra Has Began\n",
      "Downloading Spectra Has Ended\n"
     ]
    }
   ],
   "source": [
    "testcirObject.DownloadSpectra()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuum Normalization Began\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Model is linear in parameters; consider using linear fitting methods. [astropy.modeling.fitting]\n",
      "WARNING:astropy:Model is linear in parameters; consider using linear fitting methods.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuum Normalization Ended\n"
     ]
    }
   ],
   "source": [
    "testcirObject.ContinuumNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Correlate Has Began\n",
      "[-33.73333333333103, -33.59999999999769, -33.59999999999769, -33.33333333333101, -33.33333333333101, -33.33333333333101, -33.19999999999767, -33.06666666666433, -33.46666666666435, -33.73333333333103, -33.06666666666433, -33.59999999999769, -33.33333333333101, -33.33333333333101, -33.33333333333101, -33.59999999999769]\n",
      "[-32.13333333333095, -32.26666666666429, -32.26666666666429, -31.999999999997613, -32.26666666666429, -31.866666666664273, -31.999999999997613, -31.733333333330933, -32.13333333333095, -31.733333333330933, -31.733333333330933, -32.13333333333095, -31.866666666664273, -31.999999999997613, -31.999999999997613, -31.999999999997613]\n",
      "[46.66666666667297, 46.93333333333965, 46.80000000000631, 46.93333333333965, 46.66666666667297, 46.93333333333965, 46.93333333333965, 47.46666666667301, 47.06666666667299, 46.80000000000631, 46.80000000000631, 47.06666666667299, 47.06666666667299, 47.33333333333967, 47.60000000000635, 46.93333333333965]\n",
      "[8.400000000004397, 8.666666666671077, 8.533333333337737, 8.800000000004417, 8.400000000004397, 8.666666666671077, 8.666666666671077, 9.200000000004437, 8.800000000004417, 8.400000000004397, 9.200000000004437, 9.200000000004437, 8.400000000004397, 9.066666666671097, 9.066666666671097, 8.533333333337737]\n",
      "[-9.599999999996498, -9.466666666663158, -9.333333333329819, -9.199999999996479, -9.066666666663139, -8.933333333329799, -9.199999999996479, -8.799999999996459, -9.066666666663139, -9.199999999996479, -8.799999999996459, -8.799999999996459, -8.933333333329799, -8.666666666663119, -8.933333333329799, -8.533333333329779]\n",
      "[-37.86666666666457, -37.59999999999789, -37.59999999999789, -37.59999999999789, -37.59999999999789, -37.46666666666455, -37.33333333333121, -37.46666666666455, -37.59999999999789, -37.59999999999789, -37.33333333333121, -37.73333333333123, -37.59999999999789, -37.33333333333121, -37.19999999999787, -37.73333333333123]\n",
      "[7.333333333337677, 7.600000000004357, 7.333333333337677, 7.600000000004357, 7.466666666671017, 7.466666666671017, 7.600000000004357, 7.866666666671037, 7.733333333337697, 7.333333333337677, 7.866666666671037, 8.000000000004377, 7.466666666671017, 7.866666666671037, 7.866666666671037, 8.000000000004377]\n",
      "[8.800000000004417, 8.266666666671057, 8.533333333337737, 8.400000000004397, 8.400000000004397, 8.666666666671077, 8.266666666671057, 8.266666666671057, 8.266666666671057, 8.666666666671077, 8.133333333337717, 8.400000000004397, 8.533333333337737, 8.533333333337737, 8.933333333337757, 9.733333333337796]\n",
      "Cross Correlate Has Ended\n"
     ]
    }
   ],
   "source": [
    "testcirObject.CrossCorrelate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Testing Below------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### wl\n",
    "wl = pd.read_csv(\"interpolated_wl.csv\")\n",
    "wl = np.append(float(list(wl.to_dict().keys())[0]),wl.T)\n",
    "#tr_ID\n",
    "ID_and_flux = pd.read_csv(\"fluxes_for_HIRES.csv\")\n",
    "ID = ID_and_flux.columns.to_numpy()  \n",
    "# #tr_flux\n",
    "tr_flux = ID_and_flux.transpose().to_numpy() \n",
    "tr_ID = ID_and_flux.columns.to_numpy()\n",
    "#tr_ivar\n",
    "ivar = pd.read_csv(\"ivar_for_HIRES.csv\")\n",
    "tr_ivar = ivar.to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tr_ivar)):\n",
    "    if np.isnan(tr_ivar[i]).any():\n",
    "        print(f\"{i} has nan/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
